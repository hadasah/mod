
SWEEP_NAME=$1;
# Number of GPUs you'd like to train on
NUM_GPUS=$2;
# Number of nodes you'd like to train on (assuming 8 GPUs per node) -- adding 7 to round up
NUM_NODES=$(((${NUM_GPUS}+7)/8));
# Fairseq model name (e.g. transformer_lm; see https://github.com/kernelmachine/demix/blob/main/fairseq/models/transformer_lm.py for other options)
ARCH=$3;
# Baseline type: choice between demix, dense, unbalanced_dense, and domain_token
EXPERIMENT=$4;
# domain to train on 
DOMAIN=$5;
# path to top level directory to where you'd like to output the model
TOP_LEVEL_SERIALIZATION_DIR=$6;
# total number of updates
NUM_STEPS=$7
# update frequency
UPDATE_FREQ=$8
# learning rate
LR=$9
# name of wandb project to track model output (at wandb.ai)
WANDB_PROJECT=${10};
# name of this train run
MODEL_ID=${11};

if [[ $DOMAIN == *"wikipedia"* ]]; then
    DATA_PATH=/gscratch/zlab/margsli/data-bin/wikipedia-en/wikipedia-en-all/wikipedia-en-all/;
else
    DATA_PATH=/gscratch/zlab/margsli/gitfiles/demix-data/data-bin/${DOMAIN};
fi
SERIALIZATION_DIR=${TOP_LEVEL_SERIALIZATION_DIR}/${SWEEP_NAME}${MODEL_ID};

TOKENS_PER_SAMPLE=1024;
BATCH_SIZE=2;
LOG_INTERVAL=50;
KEEP_INTERVAL_UPDATES=-1;

if [[ $ARCH == *"gpt3_small"* ]]; then
     CLIP_NORM=0.1;
     SAVE_INTERVAL_UPDATES=6000;
     VALIDATION_INTERVAL=3000;
     NUM_WARMUP_STEPS=$((${NUM_STEPS} * 8 / 100));
elif [[ $ARCH == *"gpt3_medium"* ]]; then
     NUM_WARMUP_STEPS=$((${NUM_STEPS} * 8 / 100));
     SAVE_INTERVAL_UPDATES=3000;
     VALIDATION_INTERVAL=2000;
     CLIP_NORM=0.1;
elif [[ $ARCH == *"gpt3_large"* ]]; then
     NUM_WARMUP_STEPS=$((${NUM_STEPS} * 8 / 100));
     SAVE_INTERVAL_UPDATES=2000;
     VALIDATION_INTERVAL=1000;
     CLIP_NORM=0.1;
elif [[ $ARCH == *"gpt3_xl"* ]]; then
     NUM_WARMUP_STEPS=$((${NUM_STEPS} * 8 / 100));
     SAVE_INTERVAL_UPDATES=2000;
     VALIDATION_INTERVAL=500;
     CLIP_NORM=0.1;
elif [[ $ARCH == *"transformer_lm"* ]]; then
     TOKENS_PER_SAMPLE=1024;
     CLIP_NORM=0.1;
     SAVE_INTERVAL_UPDATES=12000;
     VALIDATION_INTERVAL=6000;
     NUM_WARMUP_STEPS=$((${NUM_STEPS} * 8 / 100));
fi;

fairseq-train    $DATA_PATH \
          --task language_modeling \
          --sample-break-mode none \
          --log-format simple  \
          --log-interval $LOG_INTERVAL    \
          --skip-invalid-size-inputs-valid-test     \
          --validate-interval-updates $VALIDATION_INTERVAL     \
          --save-interval-updates $SAVE_INTERVAL_UPDATES     \
          --keep-interval-updates $KEEP_INTERVAL_UPDATES    \
          --arch $ARCH    \
          --criterion cross_entropy     \
          --lr-scheduler polynomial_decay     \
          --num-workers 2 \
          --max-sentences $BATCH_SIZE \
          --no-epoch-checkpoints \
          --max-sentences-valid $BATCH_SIZE \
          --lr $LR              \
          --tokens-per-sample $TOKENS_PER_SAMPLE          \
          --optimizer adam \
          --adam-betas '(0.9, 0.95)'  \
          --adam-eps 10e-8 \
          --weight-decay 0.1 \
          --clip-norm $CLIP_NORM      \
          --max-update $NUM_STEPS     \
          --total-num-update $NUM_STEPS     \
          --warmup-updates $NUM_WARMUP_STEPS     \
          --update-freq $UPDATE_FREQ     \
          --save-dir ${SERIALIZATION_DIR}    \
          --batch-size-valid 2                        \
          --wandb-project $WANDB_PROJECT \
          --required-batch-size-multiple 1 \
          --memory-efficient-fp16 \
          --ddp-backend no_c10d \
          --all-gather-list-size 32000;
